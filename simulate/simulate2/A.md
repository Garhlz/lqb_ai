层归一化（Layer Normalization，简称 LN）是一种深度学习中的标准化技术，广泛用于加速神经网络训练并提升模型性能，尤其在处理序列数据（如 RNN、LSTM 和 Transformer）时表现优异。
其核心思想是对每个样本的特征在指定维度上进行标准化，使特征的均值为 0，方差为 1，从而稳定训练过程，减少内部协变量偏移的影响。

# 目标
现要求编写代码实现层归一化操作，完成以下函数：
**def layer_norm(X: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:**
方法，该方法的功能是对输入序列的每个样本在最后一个维度上进行层归一化。

## 函数功能：
对输入序列 X 的每个样本在最后一个维度上进行层归一化，返回标准化后的序列。

## 参数说明：
X：输入序列，一个多维 NumPy 数组，形状任意，但最后一个维度表示特征维度。
epsilon：一个小的常数，默认为 1e-5，用于避免除以零的情况。

## 返回值：
经过层归一化处理后的序列，形状与输入 X 相同。